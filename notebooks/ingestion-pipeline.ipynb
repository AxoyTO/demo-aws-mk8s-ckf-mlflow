{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb631b4-5cba-44f4-a611-e650b4c34671",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kfp[kubernetes] -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13ef2a0-e4b2-4af7-9ccc-ce1de19db50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp import kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1a09cd-d105-4820-b31e-3559d3b62e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_client = kfp.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21422a2a-b827-4ab0-be5a-b96c0269f0fd",
   "metadata": {},
   "source": [
    "# Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fc83c6-5cb4-4da4-9dfc-01c87cde8478",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image='python:3.11',\n",
    "    packages_to_install=[\"opensearch-py==2.7.1\"]\n",
    ")\n",
    "def setup_os(rag_index_name:str = \"rag_index\", force_recreate: bool = True):\n",
    "    import os\n",
    "    from opensearchpy import OpenSearch\n",
    "    \n",
    "    def delete_opensearch_index(opensearch_client, index_name):\n",
    "        print(f\"Trying to delete index {index_name}\")\n",
    "        try:\n",
    "            response = opensearch_client.indices.delete(index=index_name)\n",
    "            print(f\"Index {index_name} deleted\")\n",
    "            return response['acknowledged']\n",
    "        except Exception as e:\n",
    "            print(f\"Index {index_name} not found, nothing to delete\")\n",
    "            return True\n",
    "    \n",
    "    def create_index(opensearch_client, index_name):\n",
    "        settings = {\n",
    "            \"settings\": {\n",
    "                \"index\": {\n",
    "                    \"knn\": True\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        response = opensearch_client.indices.create(index=index_name, body=settings)\n",
    "        return bool(response['acknowledged'])\n",
    "    \n",
    "    def create_index_mapping(opensearch_client, index_name):\n",
    "        response = opensearch_client.indices.put_mapping(\n",
    "            index=index_name,\n",
    "            body={\n",
    "                \"properties\": {\n",
    "                    \"vector_field\": {\n",
    "                        \"type\": \"knn_vector\",\n",
    "                        \"dimension\": 384\n",
    "                    },\n",
    "                    \"text\": {\n",
    "                        \"type\": \"keyword\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        return bool(response['acknowledged'])\n",
    "\n",
    "    host = os.environ['OPENSEARCH_HOST']\n",
    "    port = os.environ['OPENSEARCH_PORT']\n",
    "    auth = (\n",
    "        os.environ['OPENSEARCH_USER'],\n",
    "        os.environ['OPENSEARCH_PASSWORD']\n",
    "    ) \n",
    "    \n",
    "    client = OpenSearch(\n",
    "        hosts = [{'host': host, 'port': port}],\n",
    "        http_compress = True, \n",
    "        http_auth = auth,\n",
    "        use_ssl = True,\n",
    "        verify_certs = False,\n",
    "        ssl_assert_hostname = False,\n",
    "        ssl_show_warn = False\n",
    "    )\n",
    "    \n",
    "    if force_recreate:\n",
    "        delete_opensearch_index(client, rag_index_name)\n",
    "    \n",
    "    index_exists = client.indices.exists(index=rag_index_name)\n",
    "    \n",
    "    if not index_exists:\n",
    "        print(\"Creating OpenSearch index\")\n",
    "        index_created = create_index(client, rag_index_name)\n",
    "        if index_created:\n",
    "            print(\"Creating OpenSearch index mapping\")\n",
    "            success = create_index_mapping(client, rag_index_name)\n",
    "            print(f\"OpenSearch Index mapping created\")\n",
    "    else:\n",
    "        print(\"Opensearch index already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9932da4-3caf-49ea-a287-0f55a0f5fd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image='python:3.11',\n",
    "    packages_to_install=[\"minio<7.0\"]\n",
    ")\n",
    "def download_data(bucket_name: str = \"rag-demo-source\", data_mount_point: str = \"/data\", data_folder: str = \"raw\") -> str:\n",
    "    import os\n",
    "\n",
    "    from minio import Minio\n",
    "    from minio.error import BucketAlreadyOwnedByYou, NoSuchKey\n",
    "\n",
    "    # Initialize a MinIO client\n",
    "    mc = Minio(\n",
    "        endpoint=os.environ[\"MINIO_ENDPOINT_URL\"].split(\"http://\")[1],\n",
    "        access_key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "        secret_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "        secure=False,\n",
    "    )\n",
    "\n",
    "    objects = mc.list_objects(bucket_name)\n",
    "    for obj in objects:\n",
    "        mc.fget_object(bucket_name, obj.object_name, f\"{data_mount_point}/{data_folder}/{obj.object_name}\")\n",
    "        print(\"\\t\", \"Downloaded\", obj.object_name)\n",
    "\n",
    "    return data_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b881b8-b4de-439a-9ea1-ea260cfea289",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image='python:3.11'\n",
    ")\n",
    "def remove_unsupported_files(data_source_folder:str, data_mount_point: str = \"/data\", data_target_folder:str = \"cleaned\") -> str:\n",
    "    import os\n",
    "    import shutil\n",
    "    \n",
    "    SUPPORTED_FORMATS = {\".csv\", \".doc\", \".docx\", \".enex\", \".epub\", \".html\", \".md\", \".odt\", \".pdf\", \".ppt\", \".pptx\", \".txt\",}\n",
    "    source_folder = f\"{data_mount_point}/{data_source_folder}\"\n",
    "    target_folder = f\"{data_mount_point}/{data_target_folder}\"\n",
    "\n",
    "    os.makedirs(os.path.join(data_mount_point, data_target_folder), exist_ok=True)\n",
    "    \n",
    "    for f in os.listdir(source_folder):\n",
    "        _, ext = os.path.splitext(f)\n",
    "        if ext in SUPPORTED_FORMATS:\n",
    "            shutil.copy2(f\"{source_folder}/{f}\", f\"{target_folder}/{f}\")\n",
    "            print(\"Copied file to :\", f\"{target_folder}/{f}\")\n",
    "\n",
    "    return data_target_folder\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03527d96-969e-458a-a5e5-425298d42948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dsl.component(\n",
    "#     base_image='python:3.11',\n",
    "#     packages_to_install=[\"opensearch-py==2.7.1\", \"langchain==0.2.16\", \"langchain-community==0.2.16\", \"sentence-transformers==3.0.1\"]\n",
    "# )\n",
    "@dsl.component(\n",
    "    base_image='bponieckiklotz/kfp-steps:ingestion-os',\n",
    "    packages_to_install=[\"pymupdf\", \"unstructured\"]\n",
    ")\n",
    "def ingest_os(\n",
    "    index_name:str, \n",
    "    data_source_folder:str, \n",
    "    data_mount_point: str = \"/data\",\n",
    "    use_gpu:bool = False,\n",
    "    chunk_size:int = 500,\n",
    "    chunk_overlap:int = 50,\n",
    "    batch_size:int = 10,\n",
    "    embeddings_model_name:str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "):\n",
    "    \n",
    "    from langchain.document_loaders import (\n",
    "        CSVLoader,\n",
    "        EverNoteLoader,\n",
    "        PyMuPDFLoader,\n",
    "        TextLoader,\n",
    "        UnstructuredEmailLoader,\n",
    "        UnstructuredEPubLoader,\n",
    "        UnstructuredHTMLLoader,\n",
    "        UnstructuredMarkdownLoader,\n",
    "        UnstructuredODTLoader,\n",
    "        UnstructuredPowerPointLoader,\n",
    "        UnstructuredWordDocumentLoader,\n",
    "    )\n",
    "    from langchain.docstore.document import Document\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "    from langchain.embeddings import HuggingFaceEmbeddings\n",
    "    from langchain.vectorstores import OpenSearchVectorSearch\n",
    "\n",
    "    from opensearchpy import OpenSearch\n",
    "\n",
    "    import os\n",
    "    import glob\n",
    "    \n",
    "    \n",
    "    # Map file extensions to document loaders and their arguments\n",
    "    LOADER_MAPPING = {\n",
    "        \".csv\": (CSVLoader, {}),\n",
    "        \".doc\": (UnstructuredWordDocumentLoader, {}),\n",
    "        \".docx\": (UnstructuredWordDocumentLoader, {}),\n",
    "        \".enex\": (EverNoteLoader, {}),\n",
    "        \".epub\": (UnstructuredEPubLoader, {}),\n",
    "        \".html\": (UnstructuredHTMLLoader, {}),\n",
    "        \".md\": (UnstructuredMarkdownLoader, {}),\n",
    "        \".odt\": (UnstructuredODTLoader, {}),\n",
    "        \".pdf\": (PyMuPDFLoader, {}),\n",
    "        \".ppt\": (UnstructuredPowerPointLoader, {}),\n",
    "        \".pptx\": (UnstructuredPowerPointLoader, {}),\n",
    "        \".txt\": (TextLoader, {\"encoding\": \"utf8\"}),\n",
    "        # Add more mappings for other file extensions and loaders as needed\n",
    "    }\n",
    "    \n",
    "    def load_single_document(file_path: str) -> List[Document]:  \n",
    "        ext = \".\" + file_path.rsplit(\".\", 1)[-1]\n",
    "        if ext in LOADER_MAPPING:\n",
    "            loader_class, loader_args = LOADER_MAPPING[ext]\n",
    "            loader = loader_class(file_path, **loader_args)\n",
    "            return loader.load()\n",
    "    \n",
    "        raise ValueError(f\"Unsupported file extension '{ext}'\")\n",
    "    \n",
    "    files = []\n",
    "    for ext in LOADER_MAPPING:\n",
    "        files.extend(glob.glob(f\"{data_mount_point}/{data_source_folder}/*{ext}\", recursive=True))\n",
    "    \n",
    "    files_len = len(files)\n",
    "    docs = []\n",
    "    count = 0\n",
    "    for f in files:\n",
    "        count+=1\n",
    "        print(f\"Processing file {f}, {count}/{files_len}\")\n",
    "        docs.extend(load_single_document(f))\n",
    "        \n",
    "    \n",
    "    if not docs:\n",
    "        print(\"No new documents to load\")\n",
    "        exit(0)\n",
    "    \n",
    "    print(f\"Loaded {len(docs)} new documents\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "    )  \n",
    "    texts = text_splitter.split_documents(docs)\n",
    "    print(f\"Split into {len(texts)} chunks of text (max. {chunk_size} tokens each)\")\n",
    "    \n",
    "    hfe = HuggingFaceEmbeddings(\n",
    "        model_name=embeddings_model_name,\n",
    "        model_kwargs={\"device\": \"cuda\" if use_gpu else \"cpu\"},\n",
    "    )\n",
    "\n",
    "    host = os.environ['OPENSEARCH_HOST']\n",
    "    port = os.environ['OPENSEARCH_PORT']\n",
    "    auth = (\n",
    "        os.environ['OPENSEARCH_USER'],\n",
    "        os.environ['OPENSEARCH_PASSWORD']\n",
    "    ) \n",
    "\n",
    "    opensearch_vector_search = OpenSearchVectorSearch(\n",
    "        opensearch_url = f\"https://{host}:{port}\",\n",
    "        index_name = index_name,\n",
    "        embedding_function = hfe,\n",
    "        http_auth = auth,\n",
    "        verify_certs = False,\n",
    "        ssl_assert_hostname = False,\n",
    "        ssl_show_warn = False\n",
    "    )\n",
    "\n",
    "    texts_len = len(texts)\n",
    "    for i in range(0, texts_len, batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        print(f\"Processing batch {int(i/batch_size)}/{int(texts_len/batch_size)}\")\n",
    "        opensearch_vector_search.add_texts(\n",
    "            texts=[t.page_content for t in batch],\n",
    "            ids=[f\"{t.metadata.get('ID')}_{hash(t.page_content)}\" for t in batch],\n",
    "            metadatas=[t.metadata for t in batch],\n",
    "            bulk_size=batch_size\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd40110-5a57-429b-88d6-6132e9b895cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"Ingestion Pipeline\",\n",
    "    description=\"Ingest data from S3 to OpenSearch\"\n",
    ")\n",
    "def ingestion_pipeline(\n",
    "    rag_index_name:str = \"rag_index\",\n",
    "    rag_index_force_recreate:bool = True,\n",
    "):\n",
    "    setup_os_task = setup_os(\n",
    "        rag_index_name = rag_index_name, \n",
    "        force_recreate = rag_index_force_recreate\n",
    "    )\n",
    "    kubernetes.use_secret_as_env(setup_os_task,\n",
    "                                 secret_name='opensearch-secret',\n",
    "                                 secret_key_to_env={\n",
    "                                     'username': 'OPENSEARCH_USER',\n",
    "                                     'password': 'OPENSEARCH_PASSWORD',\n",
    "                                     'host': 'OPENSEARCH_HOST',\n",
    "                                     'port': 'OPENSEARCH_PORT',\n",
    "                                 })\n",
    "    \n",
    "    pvc_data_ingestion = kubernetes.CreatePVC(\n",
    "        pvc_name_suffix='-data-ingestion',\n",
    "        access_modes=['ReadWriteMany'],\n",
    "        size='1Gi',\n",
    "        storage_class_name='microk8s-hostpath',\n",
    "    )\n",
    "    \n",
    "    download_data_task = download_data(\n",
    "        bucket_name = \"rag-demo-source\"\n",
    "    ).set_env_variable(\n",
    "        \"MINIO_ENDPOINT_URL\", os.environ[\"MINIO_ENDPOINT_URL\"]\n",
    "    ).set_caching_options(\n",
    "        enable_caching = False\n",
    "    ).after(setup_os_task)\n",
    "    kubernetes.use_secret_as_env(download_data_task,\n",
    "                                 secret_name='mlpipeline-minio-artifact',\n",
    "                                 secret_key_to_env={\n",
    "                                     'accesskey': 'AWS_ACCESS_KEY_ID',\n",
    "                                     'secretkey': 'AWS_SECRET_ACCESS_KEY',\n",
    "                                 })\n",
    "    kubernetes.mount_pvc(\n",
    "        download_data_task,\n",
    "        pvc_name=pvc_data_ingestion.outputs['name'],\n",
    "        mount_path='/data',\n",
    "    )\n",
    "\n",
    "    remove_unsupported_files_task = remove_unsupported_files(\n",
    "        data_source_folder = download_data_task.output\n",
    "    ).set_caching_options(\n",
    "        enable_caching = False\n",
    "    ).after(download_data_task)\n",
    "    kubernetes.mount_pvc(\n",
    "        remove_unsupported_files_task,\n",
    "        pvc_name=pvc_data_ingestion.outputs['name'],\n",
    "        mount_path='/data',\n",
    "    )\n",
    "\n",
    "    ingest_os_task = ingest_os(\n",
    "        index_name = rag_index_name,\n",
    "        data_source_folder = remove_unsupported_files_task.output\n",
    "    ).set_caching_options(\n",
    "        enable_caching = False\n",
    "    ).after(remove_unsupported_files_task)\n",
    "    kubernetes.mount_pvc(\n",
    "        ingest_os_task,\n",
    "        pvc_name=pvc_data_ingestion.outputs['name'],\n",
    "        mount_path='/data',\n",
    "    )\n",
    "    kubernetes.use_secret_as_env(ingest_os_task,\n",
    "                                 secret_name='opensearch-secret',\n",
    "                                 secret_key_to_env={\n",
    "                                     'username': 'OPENSEARCH_USER',\n",
    "                                     'password': 'OPENSEARCH_PASSWORD',\n",
    "                                     'host': 'OPENSEARCH_HOST',\n",
    "                                     'port': 'OPENSEARCH_PORT',\n",
    "                                 })\n",
    "\n",
    "    delete_pvc_data_ingestion = kubernetes.DeletePVC(\n",
    "        pvc_name=pvc_data_ingestion.outputs['name']\n",
    "    ).after(ingest_os_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb9a031-65bc-497e-9a97-7c1830671e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_client.create_run_from_pipeline_func(\n",
    "    ingestion_pipeline,\n",
    "    arguments={\n",
    "        \"rag_index_name\" : \"rag_index\",\n",
    "        \"rag_index_force_recreate\" : True,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9021ae-c030-4cb5-b809-1292fe231087",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=ingestion_pipeline,\n",
    "    package_path='./ingestion-pipeline.yaml',\n",
    "    pipeline_parameters={\n",
    "        \"rag_index_name\" : \"rag_index\",\n",
    "        \"rag_index_force_recreate\" : True,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e190eb26-2705-45dc-a9cb-7aa5e891d9a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4d40d9-fbcb-4210-88ee-bddf6a24b6ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
