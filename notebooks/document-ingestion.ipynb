{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5aea951d-60f9-494d-9d04-5835064aa19c",
   "metadata": {},
   "source": [
    "# Ingestion\n",
    "\n",
    "Ingestion from the bucket into the OpenSearch database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c15e3ed-7258-470d-82f0-249d7017b4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"minio<7.0\"  opensearch-py langchain langchain-community sentence-transformers\n",
    "# remove sentence-transformers if not needed!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95ab1f7-e70d-4c8a-afba-d07795ea7ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31220201-4c8a-41b5-8602-893eba22fd67",
   "metadata": {},
   "source": [
    "## Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d941460b-5d16-4349-b57b-363ff847e204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "from minio import Minio\n",
    "from minio.error import BucketAlreadyOwnedByYou, NoSuchKey\n",
    "\n",
    "BUCKET = \"rag-demo-source\"\n",
    "\n",
    "MINIO_HOST = os.environ[\"MINIO_ENDPOINT_URL\"].split(\"http://\")[1]\n",
    "\n",
    "# Initialize a MinIO client\n",
    "mc = Minio(\n",
    "    endpoint=MINIO_HOST,\n",
    "    access_key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    secret_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "    secure=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b7f289-757f-4da2-b134-5cd3fe2cbc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "TMP_DIR = f\"{tempfile.gettempdir()}/rag\"\n",
    "shutil.rmtree(TMP_DIR)\n",
    "os.makedirs(TMP_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a30593-e78e-4273-911c-9336d15e1487",
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in mc.list_objects(BUCKET, recursive=True):\n",
    "    mc.fget_object(o.bucket_name, o.object_name, f\"{TMP_DIR}/{o.object_name}\")\n",
    "    print(\"Downloaded:\", o.object_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a6c93b-7708-4d9b-8b35-d21c7a5264b3",
   "metadata": {},
   "source": [
    "## Load as Documents and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b238d68-03bc-4922-bd72-656191fb80b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import (\n",
    "    CSVLoader,\n",
    "    EverNoteLoader,\n",
    "    PyMuPDFLoader,\n",
    "    TextLoader,\n",
    "    UnstructuredEmailLoader,\n",
    "    UnstructuredEPubLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    UnstructuredODTLoader,\n",
    "    UnstructuredPowerPointLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    ")\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "# Map file extensions to document loaders and their arguments\n",
    "LOADER_MAPPING = {\n",
    "    \".csv\": (CSVLoader, {}),\n",
    "    \".doc\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \".docx\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \".enex\": (EverNoteLoader, {}),\n",
    "    \".epub\": (UnstructuredEPubLoader, {}),\n",
    "    \".html\": (UnstructuredHTMLLoader, {}),\n",
    "    \".md\": (UnstructuredMarkdownLoader, {}),\n",
    "    \".odt\": (UnstructuredODTLoader, {}),\n",
    "    \".pdf\": (PyMuPDFLoader, {}),\n",
    "    \".ppt\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".pptx\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".txt\": (TextLoader, {\"encoding\": \"utf8\"}),\n",
    "    # Add more mappings for other file extensions and loaders as needed\n",
    "}\n",
    "\n",
    "def load_single_document(\n",
    "    file_path: str,\n",
    ") -> List[Document]:  # Return a list of 'Document' objects\n",
    "    ext = \".\" + file_path.rsplit(\".\", 1)[-1]\n",
    "    if ext in LOADER_MAPPING:\n",
    "        loader_class, loader_args = LOADER_MAPPING[ext]\n",
    "        loader = loader_class(file_path, **loader_args)\n",
    "        return loader.load()\n",
    "\n",
    "    raise ValueError(f\"Unsupported file extension '{ext}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1490db-49c5-43db-b11c-8cb9ad7be612",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 50\n",
    "\n",
    "files = []\n",
    "for ext in LOADER_MAPPING:\n",
    "    files.extend(glob.glob(os.path.join(TMP_DIR, f\"**/*{ext}\"), recursive=True))\n",
    "\n",
    "files_len = len(files)\n",
    "docs = []\n",
    "count = 0\n",
    "for f in files:\n",
    "    count+=1\n",
    "    print(f\"Processing file {f}, {count}/{files_len}\")\n",
    "    docs.extend(load_single_document(f))\n",
    "    \n",
    "\n",
    "if not docs:\n",
    "    print(\"No new documents to load\")\n",
    "    exit(0)\n",
    "\n",
    "print(f\"Loaded {len(docs)} new documents\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP\n",
    ")  \n",
    "texts = text_splitter.split_documents(docs)\n",
    "print(f\"Split into {len(texts)} chunks of text (max. {CHUNK_SIZE} tokens each)\")\n",
    "# texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30b2b4f-5a70-4c0f-8b74-cc7b3bad9eba",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eecab2-ddac-4040-966b-7db20f1da8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c59135-cc50-4f2e-bd59-2fbfbd5d5659",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "use_gpu = False\n",
    "\n",
    "hfe = HuggingFaceEmbeddings(\n",
    "    model_name=embeddings_model_name,\n",
    "    model_kwargs={\"device\": \"cuda\" if use_gpu else \"cpu\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2886bdaa-4948-44e5-9358-c90335fb83f2",
   "metadata": {},
   "source": [
    "## Save to Opensearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248390c0-bb1a-4d05-b975-9798aeec6608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b05e26-8cb0-459d-ac17-328c74c3908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "host = os.environ['OPENSEARCH_HOST']\n",
    "port = os.environ['OPENSEARCH_PORT']\n",
    "auth = (\n",
    "    os.environ['OPENSEARCH_USER'],\n",
    "    os.environ['OPENSEARCH_PASSWORD']\n",
    ") \n",
    "\n",
    "client = OpenSearch(\n",
    "    hosts = [{'host': host, 'port': port}],\n",
    "    http_compress = True, \n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = False,\n",
    "    ssl_assert_hostname = False,\n",
    "    ssl_show_warn = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf92701-9af0-4f0b-ad01-5af0a50a8ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_opensearch_index(opensearch_client, index_name):\n",
    "    print(f\"Trying to delete index {index_name}\")\n",
    "    try:\n",
    "        response = opensearch_client.indices.delete(index=index_name)\n",
    "        print(f\"Index {index_name} deleted\")\n",
    "        return response['acknowledged']\n",
    "    except Exception as e:\n",
    "        print(f\"Index {index_name} not found, nothing to delete\")\n",
    "        return True\n",
    "\n",
    "def create_index(opensearch_client, index_name):\n",
    "    settings = {\n",
    "        \"settings\": {\n",
    "            \"index\": {\n",
    "                \"knn\": True\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    response = opensearch_client.indices.create(index=index_name, body=settings)\n",
    "    return bool(response['acknowledged'])\n",
    "\n",
    "def create_index_mapping(opensearch_client, index_name):\n",
    "    response = opensearch_client.indices.put_mapping(\n",
    "        index=index_name,\n",
    "        body={\n",
    "            \"properties\": {\n",
    "                \"vector_field\": {\n",
    "                    \"type\": \"knn_vector\",\n",
    "                    \"dimension\": 384\n",
    "                },\n",
    "                \"text\": {\n",
    "                    \"type\": \"keyword\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    return bool(response['acknowledged'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e81a771-7559-4ac5-8f84-8d69a9a79f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME = \"rag_index\"\n",
    "FORCE_RECREATE = True\n",
    "\n",
    "if FORCE_RECREATE:\n",
    "    delete_opensearch_index(client, INDEX_NAME)\n",
    "\n",
    "index_exists = client.indices.exists(index=INDEX_NAME)\n",
    "\n",
    "if not index_exists:\n",
    "    print(\"Creating OpenSearch index\")\n",
    "    index_created = create_index(client, INDEX_NAME)\n",
    "    if index_created:\n",
    "        print(\"Creating OpenSearch index mapping\")\n",
    "        success = create_index_mapping(client, INDEX_NAME)\n",
    "        print(f\"OpenSearch Index mapping created\")\n",
    "else:\n",
    "    print(\"Opensearch index already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9287df-2563-4469-8193-f33ade8295f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "\n",
    "opensearch_vector_search = OpenSearchVectorSearch(\n",
    "    opensearch_url = f\"https://{host}:{port}\",\n",
    "    index_name = INDEX_NAME,\n",
    "    embedding_function = hfe,\n",
    "    http_auth = auth,\n",
    "    verify_certs = False,\n",
    "    ssl_assert_hostname = False,\n",
    "    ssl_show_warn = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2755026e-cb1c-45d9-af2a-51ff485af39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "texts_len = len(texts)\n",
    "for i in range(0, texts_len, batch_size):\n",
    "    batch = texts[i:i + batch_size]\n",
    "    print(f\"Processing batch {int(i/batch_size)}/{int(texts_len/batch_size)}\")\n",
    "    opensearch_vector_search.add_texts(\n",
    "        texts=[t.page_content for t in batch],\n",
    "        ids=[f\"{t.metadata.get('ID')}_{hash(t.page_content)}\" for t in batch],\n",
    "        metadatas=[t.metadata for t in batch],\n",
    "        bulk_size=batch_size\n",
    "    )\n",
    "\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e739532-2a98-40ae-b701-5abebf1660cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "opensearch_vector_search.similarity_search(\"what is FPV drone?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de58f50-807f-46af-9651-1bfdf9aeac10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
